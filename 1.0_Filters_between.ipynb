{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "from sklearn.neighbors import BallTree\n",
    "from sklearn.linear_model import LinearRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all data\n",
    "#frame = pd.read_pickle('Data\\ANA_clean.pkl')\n",
    "#gdf = gpd.read_file('Data\\stations.geojson')\n",
    "frame = pd.read_pickle(r'Data\\all_clean.pkl')\n",
    "gdf = gpd.read_file(r'Data\\all_stations.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the thresholds\n",
    "start = '1981-01-01'\n",
    "end = '2010-12-31'\n",
    "na_lim = 20\n",
    "std_lim = 2.7\n",
    "between_limit = 0.75\n",
    "file_name = 'Data\\ALL_81_10_20bet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df(start, end, na, frame = frame):\n",
    "    \"\"\"\n",
    "    Filter the DataFrame by date and remove stations with too many missing values.\n",
    "\n",
    "    Parameters:\n",
    "        start (datetime): The starting date of the desired date range (inclusive).\n",
    "        end (datetime): The ending date of the desired date range (inclusive).\n",
    "        na (float): The threshold percentage of missing values allowed for a station to be included in the filtered DataFrame.\n",
    "        frame (pandas.DataFrame, optional): The input DataFrame to be filtered. \n",
    "\n",
    "    Returns:\n",
    "        df_final (pandas.DataFrame): The filtered DataFrame containing the stations that meet the date range criteria and have a percentage of missing values below the threshold.\n",
    "\n",
    "    \"\"\"\n",
    "  \n",
    "    date_begin = start\n",
    "    date_end = end\n",
    "    #selectt data in frame in start-end period\n",
    "    frame_dates = frame[(frame['Date'] >= start) & (frame['Date'] <= end)]\n",
    "    date_range = pd.date_range(start = date_begin, end = date_end, freq='MS' )\n",
    "    codes = frame_dates['Code'].unique()\n",
    "    df_na = frame_dates.dropna()\n",
    "    diff_list = []\n",
    "    for code in codes:\n",
    "        diff = date_range.difference(df_na[df_na['Code']==code]['Date'])\n",
    "        diff_list.append(len(diff))\n",
    "    difs_date_series = pd.Series(diff_list, index=codes)\n",
    "    values_na = difs_date_series / len(date_range) *100\n",
    "    na_codes_list= values_na[values_na < na].index.to_list()\n",
    "    df_final = df_na[df_na['Code'].isin(na_codes_list)]\n",
    "\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dates = filter_df(start, end, na_lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_raw(df, gdf = gdf):\n",
    "    \"\"\"\n",
    "    Get only raw data from the filtered DataFrame and drop duplicated stations and stations with dates out of the pattern (first day of the month).\n",
    "\n",
    "    Parameters:\n",
    "        df (pandas.DataFrame): The filtered DataFrame containing the stations and dates.\n",
    "        gdf (pandas.DataFrame, optional): The geographical DataFrame containing station information. If not provided, the function assumes a global variable named \"gdf\" exists.\n",
    "\n",
    "    Returns:\n",
    "        df_raw (pandas.DataFrame): The DataFrame containing only raw data without duplicated stations and dates out of the pattern.\n",
    "        gdf_filtered (pandas.DataFrame): The geographical DataFrame filtered to include only the stations present in df_raw.\n",
    "\n",
    "    \"\"\"\n",
    "    codes_out = df[df['Date'].dt.day != 1]['Code'].unique()\n",
    "    df = df[~df.Code.isin(codes_out)]\n",
    "    df_con = df.sort_values(by='Consistency').drop_duplicates(subset=['Code','Date'], keep='first')\n",
    "    df_raw = df_con.set_index(['Code','Date']).unstack().stack(dropna=False).reset_index()\n",
    "    codes_out = df_raw[df_raw['Date'].dt.day != 1]['Code'].unique()\n",
    "    df_raw = df_raw[~df_raw.Code.isin(codes_out)]\n",
    "    gdf_filtered = gdf[gdf['Code'].isin(df_raw['Code'].unique())]\n",
    "    gdf_filtered = gdf_filtered.sort_values(by='Code').drop_duplicates(subset=['Latitude','Longitude'], keep='first')\n",
    "    df_raw = df_raw[df_raw['Code'].isin(gdf_filtered['Code'].unique())]\n",
    "    return df_raw , gdf_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw, gdf_filtered = filter_raw(df_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_wrong_dates = df_dates[df_dates['Date'].dt.day != 1]['Code'].unique().shape[0]\n",
    "stat_dup = df_dates.Code.unique().shape[0] - df_raw.Code.unique().shape[0]  - stat_wrong_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_filtered.reset_index(inplace=True, drop=True)\n",
    "# using a projection to get the distance in meters\n",
    "gdf_filtered_dist = gdf_filtered.to_crs('ESRI:102032')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbors(src_points, candidates, k_neighbors):\n",
    "    \"\"\"Find nearest neighbors for all source points from a set of candidate points\"\"\"\n",
    "\n",
    "    # Create tree from the candidate points\n",
    "    tree = BallTree(candidates, leaf_size=40, metric='haversine')\n",
    "\n",
    "    # Find closest points and distances\n",
    "    distances, indices = tree.query(src_points, k=k_neighbors)\n",
    "\n",
    "    # Transpose to get distances and indices into arrays\n",
    "    distances = distances.transpose()\n",
    "    indices = indices.transpose()\n",
    "\n",
    "    return indices, distances\n",
    "\n",
    "\n",
    "def nearest_neighbors(left_gdf, right_gdf, k_neighbors=6):\n",
    "    \"\"\"\n",
    "    For each point in left_gdf, find closest point in right GeoDataFrame and return them.\n",
    "    \n",
    "    NOTICE: Assumes that the input Points are in WGS84 projection (lat/lon).\n",
    "    \"\"\"\n",
    "    \n",
    "    left_geom_col = left_gdf.geometry.name\n",
    "    right_geom_col = right_gdf.geometry.name\n",
    "    \n",
    "    # Ensure that index in right gdf is formed of sequential numbers\n",
    "    right = right_gdf.copy().reset_index(drop=True)\n",
    "    \n",
    "    # Parse coordinates from points and insert them into a numpy array as RADIANS\n",
    "    # Notice: should be in Lat/Lon format \n",
    "    left_radians = np.array(left_gdf[left_geom_col].apply(lambda geom: (geom.y * np.pi / 180, geom.x * np.pi / 180)).to_list())\n",
    "    right_radians = np.array(right[right_geom_col].apply(lambda geom: (geom.y * np.pi / 180, geom.x * np.pi / 180)).to_list())\n",
    "        \n",
    "    closest, dist = get_neighbors(src_points=left_radians, candidates=right_radians, k_neighbors = k_neighbors)\n",
    "        \n",
    "    return closest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_near = nearest_neighbors(gdf_filtered.to_crs('EPSG:4326'), gdf_filtered.to_crs('EPSG:4326'), k_neighbors=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers(df1, std_lim):\n",
    "    groups = df1.groupby([df1.Code, df1.Date.dt.month])['Total']\n",
    "    mean, std = groups.transform(\"mean\"), groups.transform(\"std\")\n",
    "    normalized = (df1['Total'] - mean) / std\n",
    "    df1['Normalized'] = normalized\n",
    "    outliers = df1[(df1['Normalized'] >= std_lim) | (df1['Normalized'] <= - std_lim)]\n",
    "    return outliers\n",
    "\n",
    "def outliers_neighbors(row, idx = index_near, std = std_lim):\n",
    "    code = row.Code\n",
    "    date = row.Date\n",
    "    indexx = (gdf_filtered['Code'] == code).idxmax()\n",
    "    val = gdf_filtered.iloc[idx[:, indexx]]['Code'].values\n",
    "    out_close = df_raw[(df_raw['Code'].isin(val) )& (df_raw['Date'] == date)]\n",
    "    return (out_close['Normalized'].abs() >= std).sum()\n",
    "\n",
    "def drop_outliers(df, std_lim):\n",
    "    outliers = detect_outliers(df, std_lim)\n",
    "    results_out = outliers[outliers.apply(outliers_neighbors, axis=1) == 1]\n",
    "    df_out_drop = df.drop(results_out.index)\n",
    "    return df_out_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out_drop = drop_outliers(df_raw, std_lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_outliers = (df_raw.shape[0] - df_out_drop.shape[0]) / df_raw.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out_drop.drop(columns=['Normalized'], inplace=True)\n",
    "df_out_drop = df_out_drop.set_index(['Code','Date']).unstack().stack(dropna=False).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_na = df_out_drop[df_out_drop['Total'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mean(df, values, weights, groupby):\n",
    "    '''\n",
    "    calculate the weighted mean of a dataframe\n",
    "    '''\n",
    "    df = df.copy()\n",
    "    grouped = df.groupby(groupby)\n",
    "    df['weighted_average'] = df[values] / grouped[weights].transform('sum') * df[weights]\n",
    "    return grouped['weighted_average'].sum(min_count=1) #min_count is required for Grouper objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_na_RV(row, idx = index_near):\n",
    "    '''\n",
    "    fill the missing values with the weighted mean of the nearest neighbors\n",
    "    '''\n",
    "    code = row.Code\n",
    "    date = row.Date\n",
    "    indexx = (gdf_filtered_dist['Code'] == code).idxmax()\n",
    "    val = gdf_filtered_dist.iloc[idx[:, indexx]]['Code'].values\n",
    "    # getting the nearest neighbors data\n",
    "    close = df_out_drop[(df_out_drop['Code'].isin(val)) ]\n",
    "    close = close[close.Date.dt.month == row.Date.month]\n",
    "    # calculating the weighted mean\n",
    "    \n",
    "    stat = gdf_filtered_dist.iloc[[indexx]]\n",
    "    stations = gdf_filtered_dist[(gdf_filtered_dist.Code.isin(val)) & (gdf_filtered_dist.Code != code)]\n",
    "    stations['dist'] = 1/(stations['geometry'].apply(lambda x: stat.distance(x)))\n",
    "\n",
    "    df_stat = close[close.Code == code]\n",
    "    close = close[close.Code != code]\n",
    "    close = close.merge(stations[['Code', 'dist']], on='Code')\n",
    "    mean_close = weighted_mean(close, 'Total', 'dist', 'Date')\n",
    "    # fitting the linear_model\n",
    "    x = mean_close.values\n",
    "    y = df_stat['Total'].values\n",
    "    x = x[~np.isnan(y)]\n",
    "    y = y[~np.isnan(y)]\n",
    "    y = y[~np.isnan(x)]\n",
    "    x = x[~np.isnan(x)]\n",
    "    x = x.reshape(-1, 1)\n",
    "    y = y.reshape(-1, 1)\n",
    "    target = mean_close.loc[date]\n",
    "    if np.isnan(target):\n",
    "        predict = np.nan\n",
    "    else:    \n",
    "        model = LinearRegression()\n",
    "        model.fit(x, y)\n",
    "        predict = model.predict(target.reshape(-1, 1))[0][0]\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "NA_filled_RV = df_na.apply(fill_na_RV , axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luisg\\AppData\\Local\\Temp\\ipykernel_26220\\3356163064.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_na['RV'] = NA_filled_RV\n"
     ]
    }
   ],
   "source": [
    "df_na['RV'] = NA_filled_RV\n",
    "na_clim = df_na['RV'].isnull().sum()\n",
    "na_reg = df_na.shape[0] - na_clim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filled = df_out_drop.copy()\n",
    "df_filled['Total'] = df_out_drop['Total'].fillna(df_na['RV'])\n",
    "df_filled['Total'] = df_filled.groupby(['Code', df_filled.Date.dt.month], group_keys=False)['Total'].apply(lambda x: x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_mass_curve(row, idx= index_near):\n",
    "    '''\n",
    "    fitting the linear regression model to the nearest neighbors\n",
    "    '''\n",
    "    code = row.Code\n",
    "    date = row.Date\n",
    "    indexx = (gdf_filtered_dist['Code'] == code).idxmax()\n",
    "    val = gdf_filtered_dist.iloc[idx[:, indexx]]['Code'].values\n",
    "    close = df_filled[(df_filled['Code'].isin(val)) ]\n",
    "    close = close[close.Date.dt.month == row.Date]\n",
    "    mean_close = close[close.Code != code].groupby(close.Date).Total.mean()\n",
    "    stat = gdf_filtered_dist.iloc[[indexx]]\n",
    "    stations = gdf_filtered_dist[(gdf_filtered_dist.Code.isin(val)) & (gdf_filtered_dist.Code != code)]\n",
    "    stations['dist'] = 1/(stations['geometry'].apply(lambda x: stat.distance(x)))\n",
    "    df_stat = close[close.Code == code]\n",
    "    close = close[close.Code != code]\n",
    "    close = close.merge(stations[['Code', 'dist']], on='Code')\n",
    "    mean_close = weighted_mean(close, 'Total', 'dist', 'Date')\n",
    "\n",
    "    x = mean_close.values\n",
    "    y = df_stat['Total'].values\n",
    "    x = x.reshape(-1, 1)\n",
    "    y = y.reshape(-1, 1)\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(x, y)\n",
    "# calculate the line using the model\n",
    "    yhat = model.predict(x)\n",
    "    df_stat['predict'] = yhat\n",
    "    return pd.Series(yhat.squeeze())\n",
    "#cada mes de cada estação\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luisg\\AppData\\Local\\Temp\\ipykernel_26220\\3503087231.py:1: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_cod_dates = df_filled.groupby(['Code', df_filled.Date.dt.month]).mean().reset_index().drop(columns=['Total'])\n"
     ]
    }
   ],
   "source": [
    "df_cod_dates = df_filled.groupby(['Code', df_filled.Date.dt.month]).mean().reset_index().drop(columns=['Total'])\n",
    "consist_values = df_cod_dates.apply(double_mass_curve , axis=1)\n",
    "df_test = df_filled.copy()\n",
    "df_test['month'] = df_filled.Date.dt.month\n",
    "df_test = df_test.sort_values(by=['Code', 'month'])\n",
    "df_test['predict'] = consist_values.to_numpy().flatten()\n",
    "df_test.drop(columns=['month'], inplace=True)\n",
    "df_test['predict'] = df_test['predict'].clip(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def between(a,b,x):\n",
    "    return (x >= a) & (x <= b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def between_consistency(row, inf=1-between_limit, sup=1 + between_limit):\n",
    "    value = 0\n",
    "    if between(row['predict']*inf, row['predict']*sup, row['Total']):\n",
    "        value = row['Total']\n",
    "    else:\n",
    "        value = row['predict']\n",
    "    return value  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_between = df_test.copy()\n",
    "df_between['between'] = df_between.apply(between_consistency, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "consist_data = (df_test['Total'] != df_between['Total']).sum() / df_between.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_between[['Code', 'Date', 'between']].rename(columns={'between': 'Total'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start 1981-01-01\n",
      "end 2010-12-31\n",
      "na_lim 20\n",
      "std_lim 2.7\n",
      "4622 stations after Date and NA Filter\n",
      "2 stations with broken dates\n",
      "17 duplicated stations\n",
      "0.65% dropped outliers\n",
      "66351 NA values filled with linear regression\n",
      "2026 NA values filled with climatology\n",
      "0.00% consisted data\n"
     ]
    }
   ],
   "source": [
    "print('start' , start)\n",
    "print('end' , end)\n",
    "print('na_lim', na_lim)\n",
    "print('std_lim', std_lim)\n",
    "print(df_dates.Code.unique().shape[0], 'stations after Date and NA Filter')\n",
    "print(stat_wrong_dates, 'stations with broken dates')\n",
    "print(stat_dup, 'duplicated stations')\n",
    "print(f'{dropped_outliers:.2f}% dropped outliers')\n",
    "print(na_reg, 'NA values filled with linear regression')\n",
    "print(na_clim, 'NA values filled with climatology')\n",
    "print(f'{consist_data:.2f}% consisted data')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(file_name + '.pkl')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "855b0dba54f71d4ed7a7e60790e4cb76228c800f15cabd60f322789172697f8b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
