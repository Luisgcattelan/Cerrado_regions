{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen\n",
    "import wget\n",
    "import requests\n",
    "import threading\n",
    "import asyncio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stations dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the stations dataframe\n",
    "df = pd.read_excel(r'Data_stations\\ANA_DATA\\vwEstacoes.xlsx')\n",
    "stations = df[['TipoEstacao','Codigo', 'Nome', 'Latitude', 'Longitude', 'Altitude']]\n",
    "stations = stations[stations['TipoEstacao'] == 2].drop(columns=['TipoEstacao'])\n",
    "stations.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "stations.rename(columns={'Codigo': 'Code', 'Nome': 'Name'}, inplace=True)\n",
    "stations.to_csv(r'Data_stations\\stations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the stations geodataframe\n",
    "gdf = gpd.GeoDataFrame(stations, geometry=gpd.points_from_xy(stations.Longitude, stations.Latitude, crs='epsg:4326'))\n",
    "gdf.to_file(r\"Data\\stations.geojson\", driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# download and unzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download all the ANA data\n",
    "def background(f):\n",
    "    def wrapped(*args, **kwargs):\n",
    "        return asyncio.get_event_loop().run_in_executor(None, f, *args, **kwargs)\n",
    "    return wrapped\n",
    "\n",
    "@background\n",
    "def download(stat):\n",
    "  url = 'https://www.snirh.gov.br/hidroweb/rest/api/documento/convencionais?tipo=3&documentos=' + str(stat)  \n",
    "  wget.download(url, out=r'Data_stations\\ANA_DATA\\ZIP\\all' + str(stat) + '.zip')\n",
    "\n",
    "\n",
    "\n",
    "for stat in stations:\n",
    "    download(stat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    df = pd.read_csv(file, encoding='latin-1', skiprows=12, delimiter=';', index_col=False, usecols=[0,1,2,5], decimal=',', parse_dates=[2], dayfirst=True)\n",
    "    idx = df['EstacaoCodigo'][0]\n",
    "    df.to_csv(r'Data_stations\\data\\clean\\\\' + str(idx) + '.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'Data_stations\\ANA_DATA\\ZIP'\n",
    "files = glob(os.path.join(path, '*.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for corrupt files or bad files(downloaded incorecctly)\n",
    "corrupt = []\n",
    "badfile = []\n",
    "for file in files:\n",
    "    try:\n",
    "\n",
    "        with ZipFile(file) as file1:\n",
    "                if file1.testzip() is not None:\n",
    "                    print('ruim')\n",
    "                    corrupt.append(file)\n",
    "                else:\n",
    "                    file1.extractall(path+'aaa')\n",
    "    except:\n",
    "        badfile.append(file)\n",
    "        print('bad')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(badfile))\n",
    "len(corrupt)\n",
    "\n",
    "badfiles_df = pd.DataFrame({'badfiles': badfile})\n",
    "badfiles_df.to_csv(r'Data_stations\\badiles.csv')\n",
    "corrupt_df = pd.DataFrame({'corrupt': corrupt})\n",
    "corrupt_df.to_csv(r'Data_stations\\corrupt.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## corrupt data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = pd.read_csv(r'Data_stations\\corrupt.csv')['corrupt'].to_list()\n",
    "list_corrupt = [int(i.split('data')[2].split('.')[0]) for i in corr]\n",
    "badd = pd.read_csv(r'Data_stations\\badiles.csv')['badfiles'].to_list()\n",
    "list_badd = [int(i.split('data')[2].split('.')[0]) for i in badd]\n",
    "\n",
    "for station in list_corrupt:\n",
    "    url = 'https://www.snirh.gov.br/hidroweb/rest/api/documento/convencionais?tipo=3&documentos=' + str(station)\n",
    "    r = requests.get(url)\n",
    "    with open(r'data\\corrupts\\\\' + str(station) + '.zip', \"wb\") as f:\n",
    "      f.write(r.content)\n",
    "\n",
    "for station in list_badd:\n",
    "    url = 'https://www.snirh.gov.br/hidroweb/rest/api/documento/convencionais?tipo=3&documentos=' + str(station)\n",
    "    r = requests.get(url)\n",
    "    with open(r'data\\corrupts\\\\' + str(station) + '.zip', \"wb\") as f:\n",
    "      f.write(r.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'Data_stations\\corrupts\\csv'\n",
    "files = glob(os.path.join(path, '*.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cleaning data and creating a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'Data_stations\\ANA_DATA\\CSV\\csv_raw'\n",
    "files = glob(os.path.join(path, '*.csv'))\n",
    "li = []\n",
    "for file in files:\n",
    "    df = pd.read_csv(file, encoding='latin-1', skiprows=12, delimiter=';', index_col=False, usecols=[0,1,2,5], decimal=',', parse_dates=[2], dayfirst=True)\n",
    "    idx = df['EstacaoCodigo'][0]\n",
    "    li.append(df)\n",
    "    df.to_csv(r'Data_stations\\clean\\\\' + str(idx) + '.csv', index=False)\n",
    "\n",
    "frame = pd.concat(li, axis=0, ignore_index=True)\n",
    "frame = frame.rename(columns={'EstacaoCodigo':'Code','NivelConsistencia':'Consistency', 'Data': 'Date'})\n",
    "frame.to_pickle(r'Data\\ANA_clean.pkl')   \n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('geo_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "855b0dba54f71d4ed7a7e60790e4cb76228c800f15cabd60f322789172697f8b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
